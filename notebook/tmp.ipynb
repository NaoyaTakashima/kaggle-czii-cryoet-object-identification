{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_slice = 64\n",
    "D, H, W = 184, 630, 630\n",
    "\n",
    "volume = np.random.rand(D, H, W)\n",
    "probability = np.zeros((7, D, H, W), dtype=np.float32)\n",
    "count = np.zeros((7, D, H, W), dtype=np.float32)\n",
    "pad_volume = np.pad(volume, [[0, 0], [0, 640 - H], [0, 640 - W]], mode='constant', constant_values=0)\n",
    "zz = list(range(0, D - num_slice, num_slice//2)) + [D - num_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dotdict(\n",
    "    arch ='resnet34d',\n",
    "    checkpoint=\\\n",
    "    '/kaggle/input/hengck-czii-cryo-et-weights-01/resnet34d-aug-noise-00003956.pth',\n",
    "       #  '/kaggle/input/hengck-czii-cryo-et-weights-01/resnet34d-00002300.pth',\n",
    "       # '/kaggle/input/hengck-czii-cryo-et-weights-01/00003531.pth',\n",
    "    threshold={ \n",
    "        'apo-ferritin': 0.05,\n",
    "        'beta-amylase': 0.05,\n",
    "        'beta-galactosidase': 0.05,\n",
    "        'ribosome': 0.05,\n",
    "        'thyroglobulin': 0.05,\n",
    "        'virus-like-particle': 0.05,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in zz:\n",
    "    print('\\r',f'z:{z}', end='',flush=True)\n",
    "    image = pad_volume[z:z + num_slice]\n",
    "    print(image.shape)\n",
    "    batch = dotdict(\n",
    "        image=torch.from_numpy(image).unsqueeze(0),\n",
    "    )\n",
    "    with torch.amp.autocast('cuda', enabled=True):\n",
    "        with torch.no_grad():\n",
    "            output = net(batch)\n",
    "    prob = output['particle'][0].cpu().numpy()\n",
    "    probability[:, z:z + num_slice] += prob[:, :, :H, :W]\n",
    "    count[:, z:z + num_slice] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "PARTICLE= [\n",
    "    {\n",
    "        \"name\": \"apo-ferritin\",\n",
    "        \"difficulty\": 'easy',\n",
    "        \"pdb_id\": \"4V1W\",\n",
    "        \"label\": 1,\n",
    "        \"color\": [0, 255, 0, 0],\n",
    "        \"radius\": 60,\n",
    "        \"map_threshold\": 0.0418\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"beta-amylase\",\n",
    "        \"difficulty\": 'ignore',\n",
    "        \"pdb_id\": \"1FA2\",\n",
    "        \"label\": 2,\n",
    "        \"color\": [0, 0, 255, 255],\n",
    "        \"radius\": 65,\n",
    "        \"map_threshold\": 0.035\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"beta-galactosidase\",\n",
    "        \"difficulty\": 'hard',\n",
    "        \"pdb_id\": \"6X1Q\",\n",
    "        \"label\": 3,\n",
    "        \"color\": [0, 255, 0, 255],\n",
    "        \"radius\": 90,\n",
    "        \"map_threshold\": 0.0578\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ribosome\",\n",
    "        \"difficulty\": 'easy',\n",
    "        \"pdb_id\": \"6EK0\",\n",
    "        \"label\": 4,\n",
    "        \"color\": [0, 0, 255, 0],\n",
    "        \"radius\": 150,\n",
    "        \"map_threshold\": 0.0374\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"thyroglobulin\",\n",
    "        \"difficulty\": 'hard',\n",
    "        \"pdb_id\": \"6SCJ\",\n",
    "        \"label\": 5,\n",
    "        \"color\": [0, 255, 255, 0],\n",
    "        \"radius\": 130,\n",
    "        \"map_threshold\": 0.0278\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"virus-like-particle\",\n",
    "        \"difficulty\": 'easy',\n",
    "        \"pdb_id\": \"6N4V\",\n",
    "        \"label\": 6,\n",
    "        \"color\": [0, 0, 0, 255],\n",
    "        \"radius\": 135,\n",
    "        \"map_threshold\": 0.201\n",
    "    }\n",
    "]\n",
    "\n",
    "PARTICLE_COLOR=[[0,0,0]]+[\n",
    "    PARTICLE[i]['color'][1:] for i in range(6)\n",
    "]\n",
    "PARTICLE_NAME=['none']+[\n",
    "    PARTICLE[i]['name'] for i in range(6)\n",
    "]\n",
    "\n",
    "def read_one_truth(id, overlay_dir):\n",
    "    data_list = []\n",
    "\n",
    "    json_dir = f'{overlay_dir}/{id}/Picks'\n",
    "    for p in PARTICLE_NAME[1:]:\n",
    "        json_file = f'{json_dir}/{p}.json'\n",
    "\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        num_point = len(json_data['points'])\n",
    "        loc = np.array([list(json_data['points'][i]['location'].values()) for i in range(num_point)])\n",
    "\n",
    "        for point in loc:\n",
    "            data_list.append({\"particle_type\": p, \"x\": point[0], \"y\": point[1], \"z\": point[2]})\n",
    "\n",
    "    # データフレームを作成\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "valid_dir = \"/home/naoya/kaggle/czii/input/czii-cryo-et-object-identification/train/overlay/ExperimentRuns\"\n",
    "# ディレクトリ名のみを取得\n",
    "valid_ids = [\n",
    "    name for name in os.listdir(valid_dir)\n",
    "    if os.path.isdir(os.path.join(valid_dir, name))\n",
    "]\n",
    "for id in valid_ids:\n",
    "    truth = read_one_truth(id, valid_dir) #=f'{valid_dir}/overlay/ExperimentRuns')\n",
    "    solution = pd.DataFrame(truth)\n",
    "    solution.insert(loc=0, column='id', value=np.arange(len(solution)))\n",
    "    solution.insert(loc=1, column=\"experiment\",  value=id)\n",
    "    solution.to_csv(f\"/home/naoya/kaggle/czii/input/solution/solution_{id}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def concat_csv_files(input_dir, output_file):\n",
    "    # Get list of all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    # Read and concatenate all CSV files\n",
    "    df_list = [pd.read_csv(os.path.join(input_dir, file)) for file in csv_files]\n",
    "    concatenated_df = pd.concat(df_list, axis=0)\n",
    "    \n",
    "    # Save the concatenated dataframe to a new CSV file\n",
    "    concatenated_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "input_directory = '/home/naoya/kaggle/czii/input/solution'\n",
    "output_csv = '/home/naoya/kaggle/czii/input/solution/solution.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_dir = '/home/naoya/kaggle/czii/input/solution'\n",
    "valid_ids = ['TS_6_4', 'TS_69_2', 'TS_6_6', 'TS_73_6']\n",
    "\n",
    "df_list = []\n",
    "for id in valid_ids:\n",
    "    df = pd.read_csv(f'{df_dir}/solution_{id}.csv')\n",
    "    df_list.append(df)\n",
    "\n",
    "concatenate_df = pd.concat(df_list, axis=0)\n",
    "concatenate_df.to_csv(f'{df_dir}/solution_1-4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_label_TS_23.npy', 'train_image_TS_0.npy', 'train_image_TS_14.npy', 'train_label_TS_12.npy', 'train_label_TS_3.npy', 'train_label_TS_21.npy', 'train_image_TS_20.npy', 'train_label_TS_14.npy', 'train_image_TS_23.npy', 'train_image_TS_2.npy', 'train_label_TS_11.npy', 'train_image_TS_6.npy', 'train_image_TS_3.npy', 'train_image_TS_24.npy', 'train_label_TS_9.npy', 'train_image_TS_25.npy', 'train_image_TS_17.npy', 'train_image_TS_8.npy', 'train_label_TS_13.npy', 'train_label_TS_5.npy', 'train_label_TS_8.npy', 'train_image_TS_21.npy', 'train_label_TS_19.npy', 'train_image_TS_15.npy', 'train_image_TS_5.npy', 'train_image_TS_18.npy', 'train_label_TS_26.npy', 'train_label_TS_7.npy', 'train_image_TS_22.npy', 'train_label_TS_2.npy', 'train_image_TS_4.npy', 'train_image_TS_7.npy', 'train_image_TS_19.npy', 'train_image_TS_16.npy', 'train_image_TS_12.npy', 'train_label_TS_17.npy', 'train_label_TS_18.npy', 'train_image_TS_10.npy', 'train_label_TS_0.npy', 'train_label_TS_22.npy', 'train_image_TS_9.npy', 'train_image_TS_26.npy', 'train_label_TS_1.npy', 'train_label_TS_16.npy', 'train_image_TS_1.npy', 'train_image_TS_11.npy', 'train_label_TS_4.npy', 'train_label_TS_10.npy', 'train_label_TS_15.npy', 'train_label_TS_20.npy', 'train_label_TS_6.npy', 'train_label_TS_24.npy', 'train_image_TS_13.npy', 'train_label_TS_25.npy']\n",
      "Unique labels in train_label_TS_23.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_12.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_3.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_21.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_14.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_11.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_9.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_13.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_5.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_8.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_19.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_26.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_7.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_2.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_17.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_18.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_0.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_22.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_1.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_16.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_4.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_10.npy: [0 1 2 4 5 6]\n",
      "Unique labels in train_label_TS_15.npy: [0 1 3 4 5 6]\n",
      "Unique labels in train_label_TS_20.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_6.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_24.npy: [0 1 2 3 4 5 6]\n",
      "Unique labels in train_label_TS_25.npy: [0 1 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "label_dir = '/home/naoya/kaggle/czii/input/extra_data/numpy/denoised'\n",
    "\n",
    "print(os.listdir(label_dir))\n",
    "\n",
    "for file in os.listdir(label_dir):\n",
    "    if 'label' in file:\n",
    "        label = np.load(f'{label_dir}/{file}')\n",
    "        unique_labels = np.unique(label)\n",
    "        print(f'Unique labels in {file}: {unique_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
